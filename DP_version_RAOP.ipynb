{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Acts of Pizza Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divyang Prateek and Cory Kind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing and structuring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing relevant libraries for storing and analyzing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json as js\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in JSON file of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reads the json file as a String\n",
    "data2 = open(\"train.json\").read()\n",
    "#Converts JSON string to a List of Dictionaries\n",
    "jsondata2 = js.loads(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAOP data contains a variety of predictors of different formats. This step puts variables into separate categories for text and numeric, and creates an array for the outcome we are trying to predict (\"requester_received_pizza\"). We decided it was easier to work with text and numeric variables separately at this stage.\n",
    "\n",
    "\n",
    "NOTE that the following variables are not currently imported because they require extra processing. They will be addressed at a later point, but are not required for the baseline.\n",
    "\n",
    "1) requester_subreddits_at_request (returns an array)\n",
    "\n",
    "2) unix timestamp of request (date format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric variables:  23\n",
      "Number of text variables:  7\n"
     ]
    }
   ],
   "source": [
    "#numeric variables\n",
    "numeric_variables = ['number_of_downvotes_of_request_at_retrieval',\n",
    "    'number_of_upvotes_of_request_at_retrieval',\n",
    "    'post_was_edited',\n",
    "    'request_number_of_comments_at_retrieval',\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_account_age_in_days_at_request',\n",
    "    'requester_account_age_in_days_at_retrieval',\n",
    "    'requester_days_since_first_post_on_raop_at_request',\n",
    "    'requester_days_since_first_post_on_raop_at_retrieval',\n",
    "    'requester_number_of_comments_at_request',\n",
    "    'requester_number_of_comments_at_retrieval',\n",
    "    'requester_number_of_comments_in_raop_at_request',\n",
    "    'requester_number_of_comments_in_raop_at_retrieval',\n",
    "    'requester_number_of_posts_at_request',\n",
    "    'requester_number_of_posts_at_retrieval',\n",
    "    'requester_number_of_posts_on_raop_at_request',\n",
    "    'requester_number_of_posts_on_raop_at_retrieval',\n",
    "    'requester_number_of_subreddits_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_request',\n",
    "    'requester_upvotes_minus_downvotes_at_retrieval',\n",
    "    'requester_upvotes_plus_downvotes_at_request',\n",
    "    'requester_upvotes_plus_downvotes_at_retrieval',\n",
    "    'unix_timestamp_of_request_utc']\n",
    "\n",
    "#text variables\n",
    "text_variables = ['giver_username_if_known',\n",
    "    'request_id',\n",
    "    'request_text',\n",
    "    'request_text_edit_aware',\n",
    "    'request_title',\n",
    "    'requester_user_flair',\n",
    "    'requester_username']\n",
    "\n",
    "#Creating empty data frames to store the training data\n",
    "numeric_elements = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = numeric_variables)\n",
    "text_elements = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = text_variables)\n",
    "outcome = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = ['requester_received_pizza'])\n",
    "\n",
    "#Print the number of text and numeric predictors currently included\n",
    "print \"Number of numeric variables: \", len(numeric_elements.columns)\n",
    "print \"Number of text variables: \", len(text_elements.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fill these arrays from the JSON data. Although the loop approach is less efficient at large scale, we went this direction because the number of keys varies between cases in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(jsondata2)):\n",
    "    mykeys = jsondata2[i].keys()\n",
    "    myvals = jsondata2[i].values()\n",
    "    for key, val in zip(mykeys, myvals):\n",
    "        if key in numeric_variables:\n",
    "            idx = numeric_variables.index(key)\n",
    "            numeric_elements.iloc[i, idx] = val\n",
    "        if key in text_variables:\n",
    "            idx = text_variables.index(key)\n",
    "            text_elements.iloc[i, idx] = val\n",
    "        if key == 'requester_received_pizza':\n",
    "            outcome.iloc[i,0] = val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick check on the size of these arrays - the number of columns should match the number of text and numeric predictors determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric array:\n",
      "(4040, 22)\n",
      "\n",
      "Text array:\n",
      "(4040, 7)\n",
      "\n",
      "Outcome array:\n",
      "(4040, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Output shapes of numeric, text, and outcome arrays\n",
    "print \"Numeric array:\"\n",
    "print numeric_elements.shape\n",
    "print \n",
    "\n",
    "print \"Text array:\"\n",
    "print text_elements.shape\n",
    "print\n",
    "\n",
    "print \"Outcome array:\"\n",
    "print outcome.shape\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split out a dev set from the provided training data (80/20). There is no need to separate out a test set, since that is provided by Kaggle in a separate JSON file. To compare our results to other competitors in the Kaggle competition, we will need to use that test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training cases:  3232\n",
      "Number of dev cases:  808\n"
     ]
    }
   ],
   "source": [
    "random.seed(500)\n",
    "data_size = len(jsondata2)\n",
    "dev_indices = random.sample(range(data_size), data_size / 5)\n",
    "train_indices = list(set(range(data_size)) - set(dev_indices))\n",
    "\n",
    "print \"Number of training cases: \", len(train_indices)\n",
    "print \"Number of dev cases: \", len(dev_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating features for the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the set-up is over, we can start using the text of the request to extract more interesting predictors. As a baseline, we're going to build a logistic regression model based on the word counts from the request text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pull out the request text and outcomes for training and dev sets\n",
    "train_request_text = text_elements.ix[train_indices, \"request_text\"]\n",
    "dev_request_text = text_elements.ix[dev_indices, \"request_text\"]\n",
    "\n",
    "train_outcome = outcome.ix[train_indices,].astype(int).sum(axis = 1)\n",
    "dev_outcome = outcome.ix[dev_indices,].astype(int).sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary using this basic model is:  10919\n"
     ]
    }
   ],
   "source": [
    "#Create CountVectorizer object with no preprocessing, but include basic English stop words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = \"english\")\n",
    "train_data_features = vectorizer.fit_transform(train_request_text)\n",
    "train_vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#Use train_vocab to extract the same features from the dev set\n",
    "vectorizer_dev = CountVectorizer(analyzer = \"word\", tokenizer= None, preprocessor = None, stop_words = \"english\", vocabulary = train_vocab)\n",
    "dev_data_features = vectorizer_dev.fit_transform(dev_request_text)\n",
    "\n",
    "print \"The length of the vocabulary using this basic model is: \", str(len(train_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fitting a logistic regression model and printing confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix on dev data for Logistic Regression model no processing, using only request_text: \n",
      "[[537  60]\n",
      " [156  55]]\n",
      "\n",
      "Classification report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.90      0.83       597\n",
      "          1       0.48      0.26      0.34       211\n",
      "\n",
      "avg / total       0.70      0.73      0.70       808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fit L2 Logistic Regression model\n",
    "log_regression = LogisticRegression(penalty = \"l2\", C = 1)\n",
    "log_regression.fit(train_data_features, train_outcome)\n",
    "dev_predicted_labels = log_regression.predict(dev_data_features)\n",
    "\n",
    "#Print confusion matrix and classification report\n",
    "print \"Confusion matrix on dev data for Logistic Regression model no processing, using only request_text: \"\n",
    "print metrics.confusion_matrix(dev_outcome, dev_predicted_labels, labels = [0,1])\n",
    "print\n",
    "\n",
    "print \"Classification report: \"\n",
    "print metrics.classification_report(dev_outcome, dev_predicted_labels, labels = [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are pretty happy with this as a first attempt. There is a lot of room for growth, but even with our very basic model we're seeing decent initial results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Directions for future analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check of our initial model, we pulled out the 20 unigrams with the largest weights. This can also help us think through what patterns to look for in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Features with Largest Weights\n",
      "rice\n",
      "op\n",
      "tight\n",
      "expected\n",
      "surprise\n",
      "greatest\n",
      "ones\n",
      "sound\n",
      "hurting\n",
      "million\n",
      "couch\n",
      "park\n",
      "sunday\n",
      "reasons\n",
      "pictures\n",
      "lift\n",
      "younger\n",
      "checks\n",
      "pockets\n",
      "feelin\n"
     ]
    }
   ],
   "source": [
    "#Extracting 20 largest weights from the logistic regression model and printing\n",
    "weights = log_regression.coef_\n",
    "top_weights = np.argpartition(weights[0,], -19)[-20:]\n",
    "\n",
    "#Printing out features\n",
    "print \"Unigram Features with Largest Weights\"\n",
    "for j in top_weights:\n",
    "    print str(train_vocab[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words seem directionally correct based on our early reviews of the request_text variable and the kind of requests people make. \"Rice\" is often presented as an alternative to pizza (i.e., \"I've been eating rice for a week - pizza would be a nice change\"). A second note is that \"op\" is part of the Reddit lexicon. One hypothesis is that requesters who come across as insiders are more likely to get pizza. To test for that, we can include measures like # of subreddits and length of Reddit history. A couple of other ideas we have include:\n",
    "\n",
    "- Time (seasonality, day of week, are people more likely to give at certain times of the month)\n",
    "- Text (extracting predictors from request titles, include bigrams/triagrams, # of spelling errors, potentially sentiment analysis)\n",
    "- Reddit behaviors (number of sub-reddits, length of time on Reddit, upvote/downvote differential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738861386139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyang/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:13: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "train_request_time = numeric_elements.ix[train_indices,\"unix_timestamp_of_request_utc\"].astype(long)\n",
    "train_request_dateTime = [datetime.datetime.fromtimestamp(time) for time in train_request_time]\n",
    "train_holiday_season = np.asarray([time.month >= 10 for time in train_request_dateTime]).reshape((len(train_request_time),1))\n",
    "train_holiday_label = np.asarray(train_outcome).reshape((len(train_outcome),1))\n",
    "\n",
    "dev_request_time = numeric_elements.ix[dev_indices,\"unix_timestamp_of_request_utc\"].astype(long)\n",
    "dev_request_dateTime = [datetime.datetime.fromtimestamp(time) for time in dev_request_time]\n",
    "dev_holiday_season = np.asarray([time.month >= 10 for time in dev_request_dateTime]).reshape((len(dev_request_time),1))\n",
    "dev_holiday_label = np.asarray(dev_outcome).reshape((len(dev_outcome),1))\n",
    "\n",
    "knn_clf  = KNeighborsClassifier()\n",
    "knn_clf = knn_clf.fit(train_holiday_season,train_holiday_label)\n",
    "print knn_clf.score(dev_holiday_season,dev_holiday_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2587  645]\n",
      "0.738861386139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyang/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "train_payday_effect = np.asarray([(time.day > 26 or time.day<2) for time in train_request_dateTime]).reshape((len(train_request_time),1))\n",
    "train_payday_label = np.asarray(train_outcome).reshape((len(train_outcome),1))\n",
    "\n",
    "dev_payday_effect = np.asarray([(time.day > 26 or time.day<2) for time in dev_request_dateTime]).reshape((len(dev_request_time),1))\n",
    "dev_payday_label = np.asarray(dev_outcome).reshape((len(dev_outcome),1))\n",
    "\n",
    "print np.bincount(np.asarray([(time.day > 26 or time.day<2) for time in train_request_dateTime]))\n",
    "knn_clf_pe  = KNeighborsClassifier()\n",
    "knn_clf_pe = knn_clf_pe.fit(train_payday_effect,train_payday_label)\n",
    "print knn_clf.score(dev_payday_effect,dev_payday_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
