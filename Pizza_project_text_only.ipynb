{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "name": "",
  "signature": "sha256:4f29dbd95eca7277859e10853282b7ce8fce553fe794dcdf440399d356ba8d77"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Random Acts of Pizza Baseline"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Brennan Borlaug, Divyang Prateek, and Cory Kind"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Importing and structuring data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start by importing relevant libraries for storing and analyzing data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import json as js\n",
      "import random\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.feature_extraction.text import *\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "import urllib\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from nltk import *\n",
      "from sklearn import metrics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read in JSON file of training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Reads the json file as a String\n",
      "data2 = open(\"train.json\").read()\n",
      "#Converts JSON string to a List of Dictionaries\n",
      "jsondata2 = js.loads(data2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The RAOP data contains a variety of predictors of different formats. This step puts variables into separate categories for text and numeric, and creates an array for the outcome we are trying to predict (\"requester_received_pizza\"). We decided it was easier to work with text and numeric variables separately at this stage.\n",
      "\n",
      "\n",
      "NOTE that the following variables are not currently imported because they require extra processing. They will be addressed at a later point, but are not required for the baseline.\n",
      "\n",
      "1) requester_subreddits_at_request (returns an array)\n",
      "\n",
      "2) unix timestamp of request (date format)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#numeric variables\n",
      "numeric_variables = ['number_of_downvotes_of_request_at_retrieval',\n",
      "    'number_of_upvotes_of_request_at_retrieval',\n",
      "    'post_was_edited',\n",
      "    'request_number_of_comments_at_retrieval',\n",
      "    'requester_account_age_in_days_at_request',\n",
      "    'requester_account_age_in_days_at_request',\n",
      "    'requester_account_age_in_days_at_retrieval',\n",
      "    'requester_days_since_first_post_on_raop_at_request',\n",
      "    'requester_days_since_first_post_on_raop_at_retrieval',\n",
      "    'requester_number_of_comments_at_request',\n",
      "    'requester_number_of_comments_at_retrieval',\n",
      "    'requester_number_of_comments_in_raop_at_request',\n",
      "    'requester_number_of_comments_in_raop_at_retrieval',\n",
      "    'requester_number_of_posts_at_request',\n",
      "    'requester_number_of_posts_at_retrieval',\n",
      "    'requester_number_of_posts_on_raop_at_request',\n",
      "    'requester_number_of_posts_on_raop_at_retrieval',\n",
      "    'requester_number_of_subreddits_at_request',\n",
      "    'requester_upvotes_minus_downvotes_at_request',\n",
      "    'requester_upvotes_minus_downvotes_at_retrieval',\n",
      "    'requester_upvotes_plus_downvotes_at_request',\n",
      "    'requester_upvotes_plus_downvotes_at_retrieval'\n",
      "    'unix_timestamp_of_request_utc']\n",
      "\n",
      "#text variables\n",
      "text_variables = ['giver_username_if_known',\n",
      "    'request_id',\n",
      "    'request_text',\n",
      "    'request_text_edit_aware',\n",
      "    'request_title',\n",
      "    'requester_user_flair',\n",
      "    'requester_username']\n",
      "\n",
      "#Creating empty data frames to store the training data\n",
      "numeric_elements = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = numeric_variables)\n",
      "text_elements = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = text_variables)\n",
      "outcome = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = ['requester_received_pizza'])\n",
      "\n",
      "#Print the number of text and numeric predictors currently included\n",
      "print \"Number of numeric variables: \", len(numeric_elements.columns)\n",
      "print \"Number of text variables: \", len(text_elements.columns)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of numeric variables:  22\n",
        "Number of text variables:  7\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next step is to fill these arrays from the JSON data. Although the loop approach is less efficient at large scale, we went this direction because the number of keys varies between cases in the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(jsondata2)):\n",
      "    mykeys = jsondata2[i].keys()\n",
      "    myvals = jsondata2[i].values()\n",
      "    for key, val in zip(mykeys, myvals):\n",
      "        if key in numeric_variables:\n",
      "            idx = numeric_variables.index(key)\n",
      "            numeric_elements.iloc[i, idx] = val\n",
      "        if key in text_variables:\n",
      "            idx = text_variables.index(key)\n",
      "            text_elements.iloc[i, idx] = val\n",
      "        if key == 'requester_received_pizza':\n",
      "            outcome.iloc[i,0] = val\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a quick check on the size of these arrays - the number of columns should match the number of text and numeric predictors determined above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Output shapes of numeric, text, and outcome arrays\n",
      "print \"Numeric array:\"\n",
      "print numeric_elements.shape\n",
      "print \n",
      "\n",
      "print \"Text array:\"\n",
      "print text_elements.shape\n",
      "print\n",
      "\n",
      "print \"Outcome array:\"\n",
      "print outcome.shape\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Numeric array:\n",
        "(4040, 22)\n",
        "\n",
        "Text array:\n",
        "(4040, 7)\n",
        "\n",
        "Outcome array:\n",
        "(4040, 1)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we split out a dev set from the provided training data (80/20). There is no need to separate out a test set, since that is provided by Kaggle in a separate JSON file. To compare our results to other competitors in the Kaggle competition, we will need to use that test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "random.seed(500)\n",
      "data_size = len(jsondata2)\n",
      "dev_indices = random.sample(range(data_size), data_size / 5)\n",
      "train_indices = list(set(range(data_size)) - set(dev_indices))\n",
      "\n",
      "print \"Number of training cases: \", len(train_indices)\n",
      "print \"Number of dev cases: \", len(dev_indices)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of training cases:  3232\n",
        "Number of dev cases:  808\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Creating features for the baseline model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that the set-up is over, we can start using the text of the request to extract more interesting predictors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Pull out the request text, title and outcomes for training and dev sets\n",
      "train_request_text = text_elements.ix[train_indices, \"request_text\"]\n",
      "dev_request_text = text_elements.ix[dev_indices, \"request_text\"]\n",
      "train_request_title = text_elements.ix[train_indices, \"request_title\"]\n",
      "dev_request_title = text_elements.ix[dev_indices, \"request_title\"]\n",
      "train_outcome = outcome.ix[train_indices,].astype(int).sum(axis = 1)\n",
      "dev_outcome = outcome.ix[dev_indices,].astype(int).sum(axis = 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create CountVectorizer object with no preprocessing, but include basic English stop words\n",
      "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = \"english\")\n",
      "train_data_features = vectorizer.fit_transform(train_request_text)\n",
      "train_vocab = vectorizer.get_feature_names()\n",
      "\n",
      "#Use train_vocab to extract the same features from the dev set\n",
      "vectorizer_dev = CountVectorizer(analyzer = \"word\", tokenizer= None, preprocessor = None, stop_words = \"english\", vocabulary = train_vocab)\n",
      "dev_data_features = vectorizer_dev.fit_transform(dev_request_text)\n",
      "\n",
      "print \"The length of the vocabulary using the request data alone is: \", str(len(train_vocab))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The length of the vocabulary using the request data alone is:  10919\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now I want to do the same on the titles. I expect the number to be quite a lot smaller because titles tend to be much shorter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#CountVectorizer object, same parameters, fit to title rather than text\n",
      "vectorizer_title = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = \"english\")\n",
      "train_data_features_title = vectorizer_title.fit_transform(train_request_title)\n",
      "train_vocab_title = vectorizer_title.get_feature_names()\n",
      "\n",
      "vectorizer_dev_title = CountVectorizer(analyzer = \"word\", tokenizer= None, preprocessor = None, stop_words = \"english\", vocabulary = train_vocab_title)\n",
      "dev_data_features_title = vectorizer_dev_title.fit_transform(dev_request_title)\n",
      "\n",
      "print \"The length of the vocabulary using the title alone is: \", str(len(train_vocab_title))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The length of the vocabulary using the title alone is:  3773\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, I want to include ngrams. One challenge with ngrams is that the number of phrases can quickly get out of hand if you don't impose a minimum frequency. To do that, I'm going to test the number of predictors at certain minimum thresholds to try and understand what's a reasonable minimum value for this data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minimums = [.0000001, .000001, .00001, .00005, .0001, .0005, .001, .005, .01, .05]\n",
      "\n",
      "for i in minimums:\n",
      "    vectorizer_ngram = CountVectorizer(analyzer = \"word\", tokenizer = None, ngram_range = (2, 4), preprocessor = None, stop_words = \"english\", min_df = i)\n",
      "    train_data_features_ngrams = vectorizer_ngram.fit_transform(train_request_text)\n",
      "    train_vocab_ngrams = vectorizer_ngram.get_feature_names()\n",
      "    print \"Minimum: \", i\n",
      "    print \"Number of extracted 2-4 grams: \", len(train_vocab_ngrams)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Minimum:  1e-07\n",
        "Number of extracted 2-4 grams:  274054\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1e-06\n",
        "Number of extracted 2-4 grams:  274054\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1e-05\n",
        "Number of extracted 2-4 grams:  274054\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5e-05\n",
        "Number of extracted 2-4 grams:  274054\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0001\n",
        "Number of extracted 2-4 grams:  274054\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0005\n",
        "Number of extracted 2-4 grams:  12642\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.001\n",
        "Number of extracted 2-4 grams:  2936\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.005\n",
        "Number of extracted 2-4 grams:  270\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01\n",
        "Number of extracted 2-4 grams:  94\n",
        "\n",
        "Minimum: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.05\n",
        "Number of extracted 2-4 grams:  1\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From this, it looks like .001 is a decent threshold. We get a large variety of new predictors, but still fewer than we got from just the titles in the data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer_ngram = CountVectorizer(analyzer = \"word\", tokenizer = None, ngram_range = (2, 4), preprocessor = None, stop_words = \"english\", min_df = .001)\n",
      "train_data_features_ngrams = vectorizer_ngram.fit_transform(train_request_text)\n",
      "train_vocab_ngrams = vectorizer_ngram.get_feature_names()\n",
      "\n",
      "vectorizer_dev_ngram = CountVectorizer(analyzer = \"word\", tokenizer= None, ngram_range = (2, 4), preprocessor = None, stop_words = \"english\", vocabulary = train_vocab_ngrams)\n",
      "dev_data_features_ngrams = vectorizer_dev_ngram.fit_transform(dev_request_text)\n",
      "\n",
      "print \"The length of the ngram vocabulary using the request text is: \", str(len(train_vocab_ngrams))\n",
      "\n",
      "vectorizer_ngram_title = CountVectorizer(analyzer = \"word\", tokenizer = None, ngram_range = (2, 4), preprocessor = None, stop_words = \"english\", min_df = .001)\n",
      "train_features_ngrams_title = vectorizer_ngram_title.fit_transform(train_request_title)\n",
      "train_vocab_ngrams_title = vectorizer_ngram_title.get_feature_names()\n",
      "\n",
      "vectorizer_dev_ngram_title = CountVectorizer(analyzer = \"word\", tokenizer= None, ngram_range = (2, 4), preprocessor = None, stop_words = \"english\", vocabulary = train_vocab_ngrams_title)\n",
      "dev_data_features_ngrams_title = vectorizer_dev_ngram_title.fit_transform(dev_request_title)\n",
      "\n",
      "print \"The length of the ngram vocabulary using the request title is: \", str(len(train_vocab_ngrams_title))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The length of the ngram vocabulary using the request text is:  2936\n",
        "The length of the ngram vocabulary using the request title is: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 734\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, I want to look at sentiment scoring. Rather than do the combined approach (positive - negative), I'm going to include both. It may be that people with an extremely positive or extremely negative post are more likely to get pizza than someone in the middle."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizer = RegexpTokenizer(r'\\w+')\n",
      "tokenizer.tokenize(train_request_text[0])\n",
      "\n",
      "url='http://www.unc.edu/~ncaren/haphazard/negative.txt'\n",
      "file_name='negative.txt'\n",
      "urllib.urlretrieve(url,file_name)\n",
      "url='http://www.unc.edu/~ncaren/haphazard/positive.txt'\n",
      "file_name='positive.txt'\n",
      "urllib.urlretrieve(url,file_name)\n",
      "\n",
      "pos_sent = open(\"positive.txt\").read()\n",
      "positive_words = pos_sent.split('\\n')\n",
      "\n",
      "print \"Example positive words:\"\n",
      "print positive_words[:10]\n",
      "\n",
      "neg_sent = open(\"negative.txt\").read()\n",
      "negative_words = neg_sent.split('\\n')\n",
      "\n",
      "print \"Example negative words:\"\n",
      "print negative_words[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example positive words:\n",
        "['abidance', 'abidance', 'abilities', 'ability', 'able', 'above', 'above-average', 'abundant', 'abundance', 'acceptance']\n",
        "Example negative words:\n",
        "['abandoned', 'abandonment', 'aberration', 'aberration', 'abhorred', 'abhorrence', 'abhorrent', 'abhorrently', 'abhors', 'abhors']\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score_sentiment(text):\n",
      "    positive_score = list()\n",
      "    negative_score = list()\n",
      "    for elem in text:\n",
      "        positive_counter=0\n",
      "        negative_counter = 0\n",
      "        text_processed=elem.lower()\n",
      "        words=tokenizer.tokenize(text_processed)\n",
      "        for word in words:\n",
      "            if word in positive_words:\n",
      "                positive_counter=positive_counter+1\n",
      "            if word in negative_words:\n",
      "                negative_counter = negative_counter + 1\n",
      "        if len(words) == 0:\n",
      "            positive_score.append(0)\n",
      "            negative_score.append(0)\n",
      "        else:\n",
      "            positive_score.append(positive_counter/float(len(words)))\n",
      "            negative_score.append(negative_counter/float(len(words)))\n",
      "    return np.column_stack((positive_score, negative_score))\n",
      "\n",
      "train_sentiment = score_sentiment(train_request_text) #positive and negative columns\n",
      "dev_sentiment = score_sentiment(dev_request_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "Fitting a logistic regression model and printing confusion matrix and classification report"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The loop below adds all of the additional predictor sets to the request unigrams in hopes of getting an improvement over the 0.7 accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predsets = [\"Unigram features from request text\", \"Ngram features from request text\", \"Unigram features from title\", \"Ngram features from title\", \"Sentiment\"]\n",
      "train_predsets = [train_data_features, train_data_features_ngrams, train_data_features_title, train_features_ngrams_title, train_sentiment]\n",
      "dev_predsets =  [dev_data_features, dev_data_features_ngrams, dev_data_features_title, dev_data_features_ngrams_title, dev_sentiment]\n",
      "\n",
      "for j in range(len(predsets)):\n",
      "    \n",
      "    if (j == 0):\n",
      "        train_all = train_predsets[j]\n",
      "        dev_all = dev_predsets[j]\n",
      "    elif (j == 4):\n",
      "        train_all = sp.hstack((train_predsets[0], train_predsets[4]))\n",
      "        dev_all = sp.hstack((dev_predsets[0], dev_predsets[4]))\n",
      "    else:\n",
      "        train_all = sp.hstack((train_predsets[0], train_predsets[j]), format = 'csr')\n",
      "        dev_all = sp.hstack((dev_predsets[0], dev_predsets[j]), format = 'csr')\n",
      "    \n",
      "    #Fit L2 Logistic Regression model\n",
      "    log_regression = LogisticRegression(penalty = \"l2\", C = 1)\n",
      "    log_regression.fit(train_all, train_outcome)\n",
      "    dev_predicted_labels = log_regression.predict(dev_all)\n",
      "\n",
      "    #Print confusion matrix and classification report\n",
      "    print \"Used: \", predsets[0], \" and \", predsets[j]\n",
      "    print metrics.confusion_matrix(dev_outcome, dev_predicted_labels, labels = [0,1])\n",
      "    print\n",
      "\n",
      "    print \"Classification report: \"\n",
      "    print metrics.classification_report(dev_outcome, dev_predicted_labels, labels = [0, 1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Used:  Unigram features from request text  and  Unigram features from request text\n",
        "[[537  60]\n",
        " [156  55]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.77      0.90      0.83       597\n",
        "          1       0.48      0.26      0.34       211\n",
        "\n",
        "avg / total       0.70      0.73      0.70       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Ngram features from request text\n",
        "[[532  65]\n",
        " [167  44]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.76      0.89      0.82       597\n",
        "          1       0.40      0.21      0.28       211\n",
        "\n",
        "avg / total       0.67      0.71      0.68       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Unigram features from title\n",
        "[[526  71]\n",
        " [161  50]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.77      0.88      0.82       597\n",
        "          1       0.41      0.24      0.30       211\n",
        "\n",
        "avg / total       0.67      0.71      0.68       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Ngram features from title\n",
        "[[529  68]\n",
        " [162  49]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.77      0.89      0.82       597\n",
        "          1       0.42      0.23      0.30       211\n",
        "\n",
        "avg / total       0.68      0.72      0.68       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Sentiment\n",
        "[[536  61]\n",
        " [156  55]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.77      0.90      0.83       597\n",
        "          1       0.47      0.26      0.34       211\n",
        "\n",
        "avg / total       0.70      0.73      0.70       808\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of these the best model performance was the one that used only the unigrams. Similar setup for random forest below, which is included for reference (does not outperform logistic regression)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for j in range(len(predsets)):\n",
      "    \n",
      "    if (j == 0):\n",
      "        train_all = train_predsets[j]\n",
      "        dev_all = dev_predsets[j]\n",
      "    elif (j == 4):\n",
      "        train_all = sp.hstack((train_predsets[0], train_predsets[4]))\n",
      "        dev_all = sp.hstack((dev_predsets[0], dev_predsets[4]))\n",
      "    else:\n",
      "        train_all = sp.hstack((train_predsets[0], train_predsets[j]), format = 'csr')\n",
      "        dev_all = sp.hstack((dev_predsets[0], dev_predsets[j]), format = 'csr')\n",
      "    \n",
      "    #Random forest didn't do any better\n",
      "    rf = RandomForestClassifier(n_estimators = 200)\n",
      "    rf.fit(train_all, train_outcome)\n",
      "    dev_predicted_probabilities = rf.predict_proba(dev_all)\n",
      "    dev_predicted_labels = dev_predicted_probabilities[:,1] > mean(train_outcome)\n",
      "\n",
      "    #Print confusion matrix and classification report\n",
      "    print \"Used: \", predsets[0], \" and \", predsets[j]\n",
      "    print metrics.confusion_matrix(dev_outcome, dev_predicted_labels, labels = [0,1])\n",
      "    print\n",
      "\n",
      "    print \"Classification report: \"\n",
      "    print metrics.classification_report(dev_outcome, dev_predicted_labels, labels = [0, 1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Used:  Unigram features from request text  and  Unigram features from request text\n",
        "[[421 176]\n",
        " [125  86]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.77      0.71      0.74       597\n",
        "          1       0.33      0.41      0.36       211\n",
        "\n",
        "avg / total       0.66      0.63      0.64       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Ngram features from request text\n",
        "[[429 168]\n",
        " [131  80]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.77      0.72      0.74       597\n",
        "          1       0.32      0.38      0.35       211\n",
        "\n",
        "avg / total       0.65      0.63      0.64       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Unigram features from title\n",
        "[[449 148]\n",
        " [135  76]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.77      0.75      0.76       597\n",
        "          1       0.34      0.36      0.35       211\n",
        "\n",
        "avg / total       0.66      0.65      0.65       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Ngram features from title\n",
        "[[422 175]\n",
        " [131  80]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.76      0.71      0.73       597\n",
        "          1       0.31      0.38      0.34       211\n",
        "\n",
        "avg / total       0.65      0.62      0.63       808\n",
        "\n",
        "Used: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Unigram features from request text  and  Sentiment\n",
        "[[438 159]\n",
        " [135  76]]\n",
        "\n",
        "Classification report: \n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.76      0.73      0.75       597\n",
        "          1       0.32      0.36      0.34       211\n",
        "\n",
        "avg / total       0.65      0.64      0.64       808\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 44
    }
   ],
   "metadata": {}
  }
 ]
}