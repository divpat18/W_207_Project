{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: Random Acts of Pizza Competition Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIDS W207 Project: Brennan Borlaug, Cory Kind, & Divyang Prateek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1. Importing relavent libraries/modules*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing relevant libraries for storing and analyzing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json as js\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2. Importing and structuring data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in JSON file of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read json file as string\n",
    "dl_dir = '/home/bborlaug/Downloads'\n",
    "os.chdir(dl_dir)\n",
    "data2 = open(\"train.json\").read()\n",
    "#Converts JSON string to a List of Dictionaries\n",
    "jsondata2 = js.loads(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAOP data contains a variety of predictors of different formats. The decision was made to break the predictors up into three sets in order to distribute the workload and parallelize data exploration more effectively. The three predictor types included: \n",
    "  \n",
    "1) Temporal variables - Features that deal with the time at which a request was submitted.  \n",
    "2) Requester variables - Features that describe the requester at the time of request.  \n",
    "3) Text variables - Features that describe the textual content of the request.  \n",
    "  \n",
    "The following cell breaks the raw data up into the three categories outlined above.  An outcome array containing the binary outcome of the request (i.e. \"requester_received_pizza\") is also created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of temporal features: 1\n",
      "Number of requester features: 9\n",
      "Number of text features: 7\n"
     ]
    }
   ],
   "source": [
    "#temporal_variables (related to time of request)\n",
    "temporal_variables = ['unix_timestamp_of_request_utc']\n",
    "\n",
    "#requester variables (features of requester at time of request)\n",
    "requester_variables = ['requester_account_age_in_days_at_request',\n",
    "                      'requester_days_since_first_post_on_raop_at_request',\n",
    "                      'requester_number_of_comments_at_request',\n",
    "                      'requester_number_of_comments_in_raop_at_request',\n",
    "                      'requester_number_of_posts_at_request',\n",
    "                      'requester_number_of_posts_on_raop_at_request',\n",
    "                      'requester_number_of_subreddits_at_request',\n",
    "                      'requester_upvotes_minus_downvotes_at_request',\n",
    "                      'requester_upvotes_plus_downvotes_at_request']\n",
    "\n",
    "#text variables (content of request)\n",
    "text_variables = ['giver_username_if_known',\n",
    "    'request_id',\n",
    "    'request_text',\n",
    "    'request_text_edit_aware',\n",
    "    'request_title',\n",
    "    'requester_user_flair',\n",
    "    'requester_username']\n",
    "\n",
    "#Creating empty data frames to store training data\n",
    "temporal_elements = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = temporal_variables)\n",
    "requester_elements = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = requester_variables)\n",
    "text_elements = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = text_variables)\n",
    "outcome = pd.DataFrame(np.nan, index = range(len(jsondata2)), columns = ['requester_received_pizza'])\n",
    "\n",
    "#Print the number of temporal, requester, & text predictors currently included\n",
    "print \"Number of temporal features:\", len(temporal_elements.columns)\n",
    "print \"Number of requester features:\", len(requester_elements.columns)\n",
    "print \"Number of text features:\", len(text_elements.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fill these arrays with values from thee raw JSON data. Although the loop approach is less efficient at large scale, this direction was chosen because the number of keys vary between cases in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(jsondata2)):\n",
    "    mykeys = jsondata2[i].keys()\n",
    "    myvals = jsondata2[i].values()\n",
    "    for key, val in zip(mykeys, myvals):\n",
    "        if key in temporal_variables:\n",
    "            idx = temporal_variables.index(key)\n",
    "            temporal_elements.iloc[i, idx] = val\n",
    "        if key in requester_variables:\n",
    "            idx = requester_variables.index(key)\n",
    "            requester_elements.iloc[i, idx] = val\n",
    "        if key in text_variables:\n",
    "            idx = text_variables.index(key)\n",
    "            text_elements.iloc[i, idx] = val\n",
    "        if key == 'requester_received_pizza':\n",
    "            outcome.iloc[i,0] = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick check on the size of these arrays - the number of columns should match the number of temporal, requester, and text predictors determined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal array:\n",
      "(4040, 1)\n",
      "\n",
      "Requester array:\n",
      "(4040, 9)\n",
      "\n",
      "Text array:\n",
      "(4040, 7)\n",
      "\n",
      "Outcome array:\n",
      "(4040, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Output shapes of temporal, requester, text, and outcome arrays\n",
    "print \"Temporal array:\"\n",
    "print temporal_elements.shape\n",
    "print \n",
    "\n",
    "print \"Requester array:\"\n",
    "print requester_elements.shape\n",
    "print\n",
    "\n",
    "print \"Text array:\"\n",
    "print text_elements.shape\n",
    "print\n",
    "\n",
    "print \"Outcome array:\"\n",
    "print outcome.shape\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a dev set for cross validation. Here we split out a dev set from the provided training data (80/20). There is no need to separate out a test set, since that is provided by Kaggle in a separate JSON file. To compare our results to other competitors in the Kaggle competition, we will use that test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training cases:  3232\n",
      "Number of dev cases:  808\n"
     ]
    }
   ],
   "source": [
    "random.seed(500)\n",
    "data_size = len(jsondata2)\n",
    "dev_indices = random.sample(range(data_size), data_size / 5)\n",
    "train_indices = list(set(range(data_size)) - set(dev_indices))\n",
    "\n",
    "#Define training & dev sets\n",
    "train_temporal_feats = temporal_elements.ix[train_indices,]\n",
    "train_requester_feats = requester_elements.ix[train_indices,]\n",
    "train_text_feats = requester_elements.ix[train_indices,]\n",
    "train_outcomes = outcome.ix[train_indices,].astype(int).sum(axis = 1)\n",
    "\n",
    "dev_temporal_feats = temporal_elements.ix[dev_indices,]\n",
    "dev_requester_feats = requester_elements.ix[dev_indices,]\n",
    "dev_text_feats = text_elements.ix[dev_indices,]\n",
    "dev_outcomes = outcome.ix[dev_indices,].astype(int).sum(axis = 1)\n",
    "\n",
    "print \"Number of training cases: \", len(train_indices)\n",
    "print \"Number of dev cases: \", len(dev_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3. Generate Baseline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin to develop a predictive model, we must establish a baseline to give ourselves a sense of what we are trying to achieve. In our application, the following models would serve as an effective baseline: \n",
    "    1. A model that always predicts the most frequent label (in this case, no pizza).\n",
    "    2. A model that predicts outcomes with the probability of the mean response (e.g. 26.6% of requesters receive a pizza, therefore the model will predict positive outcomes 24.6% of the time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline 1 Accuracy: 75.4 %\n",
      "Baseline 2 Accuracy: 62.6 %\n"
     ]
    }
   ],
   "source": [
    "y = 0\n",
    "r = 0\n",
    "outcomes = []\n",
    "for request in jsondata2:\n",
    "    if request['requester_received_pizza'] == True:\n",
    "        y+=1\n",
    "        r+=1\n",
    "        outcomes.append(1)\n",
    "    else:\n",
    "        r+=1\n",
    "        outcomes.append(0)\n",
    "avg = float(y)/float(r)\n",
    "\n",
    "#Baseline 1\n",
    "base1 = [0]*len(jsondata2)\n",
    "c = 0\n",
    "n = 0\n",
    "for i, j in zip(base1, outcomes):\n",
    "    if i == j:\n",
    "        c+=1\n",
    "        n+=1\n",
    "    else:\n",
    "        n+=1\n",
    "print 'Baseline 1 Accuracy:', round(float(c)/float(n),4)*100, '%'\n",
    "\n",
    "#Baseline 2\n",
    "base2 = np.random.binomial(1, avg, size=len(jsondata2))\n",
    "c = 0\n",
    "n = 0\n",
    "for i, j in zip(base2, outcomes):\n",
    "    if i == j:\n",
    "        c+=1\n",
    "        n+=1\n",
    "    else:\n",
    "        n+=1        \n",
    "print 'Baseline 2 Accuracy:', round(float(c)/float(n),4)*100, '%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *4. Explore Various Classifiers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define a function that calculates the accuracy for a number of machine learning classifiers. These include:  \n",
    "\n",
    "* Nearest Neighbors (w/ 1-15 neighbors)   \n",
    "* Logistic Regression  \n",
    "* Random Forest (w/ 1-30 estimators)  \n",
    "  \n",
    "This function can be used to explore the effectiveness of these classifiers on the RAOP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_classifiers(train_data,train_labels, dev_data,dev_labels):\n",
    "    #Nearest Neighbour Classifier\\\n",
    "    knn_scores=[]\n",
    "    knn_estimators=[]\n",
    "    for k in range(1,16,2):\n",
    "        knn = KNeighborsClassifier(algorithm='auto', n_neighbors=k)\n",
    "        knn.fit(train_data, train_labels)\n",
    "        knn_preds = knn.predict(dev_data)\n",
    "        knn_scores.append(round(metrics.accuracy_score(dev_outcomes, knn_preds),4)*100)\n",
    "        knn_estimators.append(k)\n",
    "    \n",
    "    print 'Nearest Neighbors Model Accuracy (',knn_estimators[knn_scores.index(max(knn_scores))],'neighbors):',max(knn_scores),'%'\n",
    "    \n",
    "    #Nearest Centroid Classifier\n",
    "    nc = NearestCentroid()\n",
    "    nc.fit(train_data, train_labels)\n",
    "    nc_preds = nc.predict(dev_data)\n",
    "    \n",
    "    print 'Nearest Centroid Model Accuracy:',round(metrics.accuracy_score(dev_labels, nc_preds),4)*100,'%'\n",
    "    \n",
    "    #Logistic Regression\n",
    "    reg = LogisticRegression(penalty=\"l2\", C=1.0)\n",
    "    reg.fit(train_data, train_labels)\n",
    "    reg_preds = reg.predict(dev_data)\n",
    "    \n",
    "    print 'Logistic Regression Model Accuracy:',round(metrics.accuracy_score(dev_labels, reg_preds),4)*100,'%'\n",
    "\n",
    "    estimators=[]\n",
    "    accuracies=[]\n",
    "\n",
    "    for i in range(1,30):\n",
    "        rf = RandomForestClassifier(n_estimators=i, random_state=99)\n",
    "        rf.fit(train_requester_feats, train_outcomes)\n",
    "        rf_preds=rf.predict(dev_requester_feats)\n",
    "        acc = metrics.accuracy_score(dev_outcomes, rf_preds)\n",
    "        estimators.append(i)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    max_acc = max(accuracies)\n",
    "    est = estimators[accuracies.index(max_acc)]\n",
    "    print 'Random Forests Model Accuracy (',est,'estimators ):',round(max_acc,4)*100,'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-1. KNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KNN(train_feats, train_labels, dev_feats, dev_labels, metric, weights='uniform'):\n",
    "    k = np.arange(20)+1\n",
    "    parameters = {'n_neighbors': k}\n",
    "    knn = KNeighborsClassifier(algorithm='auto', metric=metric, n_neighbors=k)\n",
    "    knn_clf = GridSearchCV(knn, parameters, cv=10)\n",
    "    knn_clf.fit(train_feats, train_labels)\n",
    "\n",
    "    k = knn_clf.best_params_['n_neighbors']\n",
    "\n",
    "    knn = KNeighborsClassifier(algorithm='auto', metric=metric, n_neighbors=k)\n",
    "    knn.fit(train_feats, train_labels)\n",
    "    knn_preds = knn.predict(dev_feats)\n",
    "    acc = round(metrics.accuracy_score(dev_labels, knn_preds),4)*100\n",
    "\n",
    "    print 'Nearest Neighbors Model Accuracy (',k,'neighbors ):',acc,'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *4-1-1. Temporal KNN Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert unix timestamp to datetime\n",
    "train_request_time = temporal_elements.ix[train_indices,\"unix_timestamp_of_request_utc\"].astype(long)\n",
    "train_request_dateTime = [datetime.datetime.fromtimestamp(time) for time in train_request_time]\n",
    "dev_request_time = temporal_elements.ix[dev_indices,\"unix_timestamp_of_request_utc\"].astype(long)\n",
    "dev_request_dateTime = [datetime.datetime.fromtimestamp(time) for time in dev_request_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in developing a model capable of predicting the outcome of a new RAOP request or generating our own request with a high probability of sucess, a few temporal features will not be useful to us (like year since all requests in the training set were made between 2011 & 2013 and we are more interested in developing a model that will generalize to current and future requests than one that excels at predicting past outcomes). As such we will focus on the following temporal variables:  \n",
    "* Day of week (dummies)  \n",
    "* Month (dummies)  \n",
    "* Hour in day (dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   features                coefs\n",
      "25        6     [0.489011292165]\n",
      "27        8     [0.356955853137]\n",
      "37       18     [0.338926368015]\n",
      "28        9     [0.320022143069]\n",
      "12      Jun     [0.278230243056]\n",
      "30       11     [0.275641923736]\n",
      "31       12     [0.208599447692]\n",
      "26        7     [0.204593782113]\n",
      "29       10     [0.159626773017]\n",
      "10      Apr     [0.122908276108]\n",
      "35       16    [0.0962172862899]\n",
      "8       Feb    [0.0375776860636]\n",
      "33       14    [0.0210668699712]\n",
      "42       23   [0.00919845490355]\n",
      "40       21  [-0.00686342452334]\n",
      "0       Mon   [-0.0133678020977]\n",
      "3       Thu   [-0.0156104088592]\n",
      "18      Dec    [-0.029851157286]\n",
      "13      Jul   [-0.0403539984815]\n",
      "7       Jan   [-0.0478147771957]\n",
      "36       17   [-0.0633480554368]\n",
      "11      May   [-0.0721255055275]\n",
      "38       19    [-0.073591824871]\n",
      "9       Mar   [-0.0912851159366]\n",
      "2       Wed    [-0.104775506167]\n",
      "39       20     [-0.12784898838]\n",
      "32       13    [-0.129730459723]\n",
      "6       Sun    [-0.129990770321]\n",
      "21        2    [-0.151261970969]\n",
      "4       Fri    [-0.183687657366]\n",
      "16      Oct    [-0.189775920286]\n",
      "5       Sat    [-0.199817719683]\n",
      "34       15     [-0.20863082988]\n",
      "24        5    [-0.238270747562]\n",
      "17      Nov    [-0.293438544426]\n",
      "1       Tue    [-0.307119225468]\n",
      "14      Aug     [-0.31056128466]\n",
      "15      Sep     [-0.31787899139]\n",
      "19        0    [-0.383984819269]\n",
      "22        3    [-0.412315421054]\n",
      "23        4    [-0.447421718835]\n",
      "41       22    [-0.502902967002]\n",
      "20        1    [-0.688058056563]\n"
     ]
    }
   ],
   "source": [
    "#Create outcome arrays\n",
    "train_time_label = np.asarray(train_outcomes)\n",
    "dev_time_label = np.asarray(train_outcomes)\n",
    "\n",
    "#Create day_of_week variable (train & dev)\n",
    "train_dow=[]\n",
    "dev_dow=[]\n",
    "for day in train_request_dateTime:\n",
    "    train_dow.append(day.weekday())\n",
    "    \n",
    "for day in dev_request_dateTime:\n",
    "    dev_dow.append(day.weekday())\n",
    "\n",
    "train_dow = np.asarray(train_dow)\n",
    "dev_dow = np.asarray(dev_dow)\n",
    "dow_train_dummies = pd.get_dummies(train_dow.flatten())\n",
    "dow_train_dummies.columns=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_dev_dummies = pd.get_dummies(dev_dow.flatten())\n",
    "dow_dev_dummies.columns=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    \n",
    "#Create month variables\n",
    "train_month = np.asarray([time.month for time in train_request_dateTime]).reshape((len(train_request_time),1))\n",
    "dev_month = np.asarray([time.month for time in dev_request_dateTime]).reshape((len(dev_request_time),1))\n",
    "\n",
    "month_train_dummies = pd.get_dummies(train_month.flatten())\n",
    "month_train_dummies.columns=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul',\n",
    "                             'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_dev_dummies = pd.get_dummies(dev_month.flatten())\n",
    "month_dev_dummies.columns=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul',\n",
    "                             'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "#Create hour_of_day variable (train & dev)\n",
    "train_hour = np.asarray([time.hour for time in train_request_dateTime]).reshape((len(train_request_time),1))\n",
    "dev_hour = np.asarray([time.hour for time in train_request_dateTime]).reshape((len(train_request_time), 1))\n",
    "\n",
    "hour_train_dummies = pd.get_dummies(train_hour.flatten())\n",
    "hour_dev_dummies = pd.get_dummies(dev_hour.flatten())\n",
    "\n",
    "#Combine all features\n",
    "temporal_train_dummies = pd.concat([dow_train_dummies,month_train_dummies,hour_train_dummies], axis=1, join='inner')\n",
    "temporal_dev_dummies = pd.concat([dow_dev_dummies,month_dev_dummies,hour_dev_dummies], axis=1, join='inner')\n",
    "\n",
    "#Create temporal feats dataframes\n",
    "train_temporal = pd.DataFrame(temporal_train_dummies)\n",
    "dev_temporal = pd.DataFrame(temporal_dev_dummies)\n",
    "\n",
    "#Check logistic regression coeffs for all feats\n",
    "lr_clf = LogisticRegression(C=1)\n",
    "lr_clf = lr_clf.fit(train_temporal,train_outcomes)\n",
    "print pd.DataFrame(zip(train_temporal, np.transpose(lr_clf.coef_)), \n",
    "                   columns=['features', 'coefs']).sort_values('coefs', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors Model Accuracy ( 20 neighbors ): 73.39 %\n"
     ]
    }
   ],
   "source": [
    "KNN(train_temporal, train_outcomes, dev_temporal, dev_outcomes, metric = \"hamming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reduce the number of temporal features to contain the 20 with the largest effect on the outcome (largest absolute value of regression coeffs) to to see if it improves accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors Model Accuracy ( 14 neighbors ): 73.76 %\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(C=1)\n",
    "lr_clf = lr_clf.fit(train_temporal,train_outcomes)\n",
    "rm = train_temporal.shape[1]-20\n",
    "removed = []\n",
    "for feat in pd.DataFrame(zip(train_temporal, np.transpose(abs(lr_clf.coef_))),\n",
    "                         columns=['features', 'coefs']).sort_values('coefs', ascending = False)['features'].tail(rm):\n",
    "    removed.append(feat)\n",
    "reduced_train_temporal = train_temporal\n",
    "reduced_dev_temporal = dev_temporal\n",
    "for feat in removed:\n",
    "    reduced_train_temporal.drop(feat, axis=1, inplace=True)\n",
    "    reduced_dev_temporal.drop(feat, axis=1, inplace=True)\n",
    "KNN(reduced_train_temporal, train_outcomes, reduced_dev_temporal, dev_outcomes, metric=\"hamming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-1-2. Requester KNN Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since requester features contain different scales, it is a good idea to normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create new dataframe to store scaled training features\n",
    "norm_train_req = pd.DataFrame()\n",
    "norm_train_req['requester_account_age_in_days_at_request'] = preprocessing.scale(train_requester_feats['requester_account_age_in_days_at_request'])\n",
    "norm_train_req['requester_days_since_first_post_on_raop_at_request'] = preprocessing.scale(train_requester_feats['requester_days_since_first_post_on_raop_at_request'])\n",
    "norm_train_req['requester_number_of_comments_at_request'] = preprocessing.scale(train_requester_feats['requester_number_of_comments_at_request'])\n",
    "norm_train_req['requester_number_of_comments_in_raop_at_request'] = preprocessing.scale(train_requester_feats['requester_number_of_comments_in_raop_at_request'])                      \n",
    "norm_train_req['requester_number_of_posts_at_request'] = preprocessing.scale(train_requester_feats['requester_number_of_posts_at_request'])    \n",
    "norm_train_req['requester_number_of_posts_on_raop_at_request'] = preprocessing.scale(train_requester_feats['requester_number_of_posts_on_raop_at_request'])                      \n",
    "norm_train_req['requester_number_of_subreddits_at_request'] = preprocessing.scale(train_requester_feats['requester_number_of_subreddits_at_request'])    \n",
    "norm_train_req['requester_upvotes_minus_downvotes_at_request'] = preprocessing.scale(train_requester_feats['requester_upvotes_minus_downvotes_at_request'])\n",
    "norm_train_req['requester_upvotes_plus_downvotes_at_request'] = preprocessing.scale(train_requester_feats['requester_upvotes_plus_downvotes_at_request'])                     \n",
    "\n",
    "#Create dataframe for scaled dev features\n",
    "norm_dev_req = pd.DataFrame()\n",
    "norm_dev_req['requester_account_age_in_days_at_request'] = preprocessing.scale(dev_requester_feats['requester_account_age_in_days_at_request'])\n",
    "norm_dev_req['requester_days_since_first_post_on_raop_at_request'] = preprocessing.scale(dev_requester_feats['requester_days_since_first_post_on_raop_at_request'])\n",
    "norm_dev_req['requester_number_of_comments_at_request'] = preprocessing.scale(dev_requester_feats['requester_number_of_comments_at_request'])\n",
    "norm_dev_req['requester_number_of_comments_in_raop_at_request'] = preprocessing.scale(dev_requester_feats['requester_number_of_comments_in_raop_at_request'])                      \n",
    "norm_dev_req['requester_number_of_posts_at_request'] = preprocessing.scale(dev_requester_feats['requester_number_of_posts_at_request'])    \n",
    "norm_dev_req['requester_number_of_posts_on_raop_at_request'] = preprocessing.scale(dev_requester_feats['requester_number_of_posts_on_raop_at_request'])                      \n",
    "norm_dev_req['requester_number_of_subreddits_at_request'] = preprocessing.scale(dev_requester_feats['requester_number_of_subreddits_at_request'])    \n",
    "norm_dev_req['requester_upvotes_minus_downvotes_at_request'] = preprocessing.scale(dev_requester_feats['requester_upvotes_minus_downvotes_at_request'])\n",
    "norm_dev_req['requester_upvotes_plus_downvotes_at_request'] = preprocessing.scale(dev_requester_feats['requester_upvotes_plus_downvotes_at_request'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            features                coefs\n",
      "3    requester_number_of_comments_in_raop_at_request     [0.255744093693]\n",
      "5       requester_number_of_posts_on_raop_at_request     [0.173259943227]\n",
      "6          requester_number_of_subreddits_at_request     [0.064288743157]\n",
      "0           requester_account_age_in_days_at_request    [0.0563296974155]\n",
      "1  requester_days_since_first_post_on_raop_at_req...    [0.0416914310184]\n",
      "7       requester_upvotes_minus_downvotes_at_request   [0.00742909288698]\n",
      "8        requester_upvotes_plus_downvotes_at_request  [-0.00789511737875]\n",
      "2            requester_number_of_comments_at_request   [-0.0272276840178]\n",
      "4               requester_number_of_posts_at_request   [-0.0389159537653]\n"
     ]
    }
   ],
   "source": [
    "#Check requester regression coeffs\n",
    "lr_clf = LogisticRegression(C=1)\n",
    "lr_clf = lr_clf.fit(norm_train_req,train_outcomes)\n",
    "print pd.DataFrame(zip(norm_train_req, np.transpose(lr_clf.coef_)), \n",
    "                   columns=['features', 'coefs']).sort_values('coefs', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors Model Accuracy ( 20 neighbors ): 74.01 %\n"
     ]
    }
   ],
   "source": [
    "KNN(norm_train_req, train_outcomes, norm_dev_req, dev_outcomes, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-1-3. Text KNN Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pull out the request text and outcomes for training and dev sets\n",
    "train_request_text = text_elements.ix[train_indices, \"request_text\"]\n",
    "dev_request_text = text_elements.ix[dev_indices, \"request_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary using this basic model is:  10919\n"
     ]
    }
   ],
   "source": [
    "#Create CountVectorizer object with no preprocessing, but include basic English stop words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = \"english\")\n",
    "train_text_features = vectorizer.fit_transform(train_request_text).toarray()\n",
    "train_vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#Use train_vocab to extract the same features from the dev set\n",
    "vectorizer_dev = CountVectorizer(analyzer = \"word\", tokenizer= None, preprocessor = None, stop_words = \"english\", vocabulary = train_vocab)\n",
    "dev_text_features = vectorizer_dev.fit_transform(dev_request_text).toarray()\n",
    "\n",
    "print \"The size of the vocabulary using this basic model is: \", str(len(train_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use PCA to reduce # of dimensions (n_components = 214 from Cory's nb)\n",
    "pca = PCA(n_components=214)\n",
    "train_text_reduced = pca.fit_transform(train_text_features)\n",
    "dev_text_reduced = pca.fit_transform(dev_text_features)\n",
    "train_text_reduced = pd.DataFrame(train_text_reduced)\n",
    "dev_text_reduced = pd.DataFrame(dev_text_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Scale new reduced text features\n",
    "cols = list(train_text_reduced.columns)\n",
    "train_text_reduced_scaled = pd.DataFrame()\n",
    "\n",
    "for col in cols:\n",
    "    col_zscore = str(col) + '_zscore'\n",
    "    train_text_reduced_scaled[col_zscore] = (train_text_reduced[col] - train_text_reduced[col].mean())/train_text_reduced[col].std(ddof=0)\n",
    "    \n",
    "cols = list(dev_text_reduced.columns)\n",
    "dev_text_reduced_scaled = pd.DataFrame()\n",
    "\n",
    "for col in cols:\n",
    "    col_zscore = str(col) + '_zscore'\n",
    "    dev_text_reduced_scaled[col_zscore] = (dev_text_reduced[col] - dev_text_reduced[col].mean())/dev_text_reduced[col].std(ddof=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors Model Accuracy ( 10 neighbors ): 73.89 %\n"
     ]
    }
   ],
   "source": [
    "KNN(train_text_reduced_scaled, train_outcomes, dev_text_reduced_scaled, dev_outcomes, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-1-4. Combine the three KNN models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combine KNN Model Accuracy: 73.89 %\n"
     ]
    }
   ],
   "source": [
    "#Optimal temporal KNN model\n",
    "knn = KNeighborsClassifier(algorithm='auto', metric=\"hamming\", n_neighbors=20)\n",
    "knn.fit(train_temporal, train_outcomes)\n",
    "temp_knn_preds = knn.predict(dev_temporal)\n",
    "\n",
    "#Optimal requester KNN model\n",
    "knn = KNeighborsClassifier(algorithm='auto', metric=\"euclidean\", n_neighbors=20)\n",
    "knn.fit(norm_train_req, train_outcomes)\n",
    "req_knn_preds = knn.predict(norm_dev_req)\n",
    "\n",
    "#Optimal text KNN model\n",
    "knn = KNeighborsClassifier(algorithm='auto', metric=\"euclidean\", n_neighbors=10)\n",
    "knn.fit(train_text_reduced_scaled, train_outcomes)\n",
    "text_knn_preds = knn.predict(dev_text_reduced_scaled)\n",
    "\n",
    "#Combine the three models\n",
    "KNN_preds=[]\n",
    "for i,j,k in zip(temp_knn_preds, req_knn_preds, text_knn_preds):\n",
    "    pred=[]\n",
    "    pred.append(i), pred.append(j), pred.append(k)\n",
    "    KNN_preds.append(max(set(pred)))\n",
    "    \n",
    "print 'Combined KNN Model Accuracy:', round(metrics.accuracy_score(dev_outcomes, KNN_preds),4)*100, '%'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the three models showed no improvement..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-2. Logistic Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-3. Random Forest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Random_Forest(train_feats, train_labels, dev_feats, dev_labels):\n",
    "    estimators=[]\n",
    "    accuracies=[]\n",
    "\n",
    "    for i in range(1,30):\n",
    "        rf = RandomForestClassifier(n_estimators=i, random_state=99)\n",
    "        rf.fit(train_feats, train_labels)\n",
    "        rf_preds=rf.predict(dev_feats)\n",
    "        acc = metrics.accuracy_score(dev_labels, rf_preds)\n",
    "        estimators.append(i)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "    max_acc = max(accuracies)\n",
    "    est = estimators[accuracies.index(max_acc)]\n",
    "    print 'Random Forests Model Accuracy (',est,'estimators ):',round(max_acc,4)*100,'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-3-1. Temporal Random Forest Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Random Forest Model:\n",
      "Random Forests Model Accuracy ( 3 estimators ): 73.51 %\n"
     ]
    }
   ],
   "source": [
    "print 'Temporal Random Forest Model:'\n",
    "Random_Forest(train_temporal, train_outcomes, dev_temporal, dev_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-3-2. Requester Random Forest Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requester Random Forest Model (simple):\n",
      "Random Forests Model Accuracy ( 26 estimators ): 73.64 %\n"
     ]
    }
   ],
   "source": [
    "print 'Requester Random Forest Model (simple):'\n",
    "Random_Forest(train_requester_feats, train_outcomes, dev_requester_feats, dev_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requester Random Forest Model (binary, scaled):\n",
      "Random Forests Model Accuracy ( 3 estimators ): 74.01 %\n"
     ]
    }
   ],
   "source": [
    "#Convert normalized feats to binary feats\n",
    "#Start with training set\n",
    "bin_train_req_1 = []\n",
    "bin_train_req_2 = []\n",
    "bin_train_req_3 = []\n",
    "bin_train_req_4 = []\n",
    "bin_train_req_5 = []\n",
    "bin_train_req_6 = []\n",
    "bin_train_req_7 = []\n",
    "bin_train_req_8 = []\n",
    "bin_train_req_9 = []\n",
    "\n",
    "for a,b,c,d,e,f,g,h,i in zip(norm_train_req['requester_account_age_in_days_at_request'],\n",
    "                             norm_train_req['requester_days_since_first_post_on_raop_at_request'],\n",
    "                             norm_train_req['requester_number_of_comments_at_request'],\n",
    "                             norm_train_req['requester_number_of_comments_in_raop_at_request'],\n",
    "                             norm_train_req['requester_number_of_posts_at_request'],\n",
    "                             norm_train_req['requester_number_of_posts_on_raop_at_request'],\n",
    "                             norm_train_req['requester_number_of_subreddits_at_request'],\n",
    "                             norm_train_req['requester_upvotes_minus_downvotes_at_request'],\n",
    "                             norm_train_req['requester_upvotes_plus_downvotes_at_request']):\n",
    "    if a>0:\n",
    "        bin_train_req_1.append(1)\n",
    "    else:\n",
    "        bin_train_req_1.append(0)\n",
    "        \n",
    "    if b>0:\n",
    "        bin_train_req_2.append(1)\n",
    "    else:\n",
    "        bin_train_req_2.append(0)\n",
    "        \n",
    "    if c>0:\n",
    "        bin_train_req_3.append(1)\n",
    "    else:\n",
    "        bin_train_req_3.append(0)\n",
    "        \n",
    "    if d>0:\n",
    "        bin_train_req_4.append(1)\n",
    "    else:\n",
    "        bin_train_req_4.append(0)\n",
    "        \n",
    "    if e>0:\n",
    "        bin_train_req_5.append(1)\n",
    "    else:\n",
    "        bin_train_req_5.append(0)\n",
    "        \n",
    "    if f>0:\n",
    "        bin_train_req_6.append(1)\n",
    "    else:\n",
    "        bin_train_req_6.append(0)\n",
    "        \n",
    "    if g>0:\n",
    "        bin_train_req_7.append(1)\n",
    "    else:\n",
    "        bin_train_req_7.append(0)\n",
    "        \n",
    "    if h>0:\n",
    "        bin_train_req_8.append(1)\n",
    "    else:\n",
    "        bin_train_req_8.append(0)\n",
    "        \n",
    "    if i>0:\n",
    "        bin_train_req_9.append(1)\n",
    "    else:\n",
    "        bin_train_req_9.append(0)\n",
    "                             \n",
    "\n",
    "bin_train_req = pd.DataFrame()\n",
    "\n",
    "bin_train_req['requester_account_age_in_days_at_request'] = bin_train_req_1\n",
    "bin_train_req['requester_days_since_first_post_on_raop_at_request'] = bin_train_req_2\n",
    "bin_train_req['requester_number_of_comments_at_request'] = bin_train_req_3\n",
    "bin_train_req['requester_number_of_comments_in_raop_at_request'] = bin_train_req_4\n",
    "bin_train_req['requester_number_of_posts_at_request'] = bin_train_req_5\n",
    "bin_train_req['requester_number_of_posts_on_raop_at_request'] = bin_train_req_6\n",
    "bin_train_req['requester_number_of_subreddits_at_request'] = bin_train_req_7\n",
    "bin_train_req['requester_upvotes_minus_downvotes_at_request'] = bin_train_req_8\n",
    "bin_train_req['requester_upvotes_plus_downvotes_at_request'] = bin_train_req_9\n",
    "\n",
    "#Now the dev set\n",
    "bin_dev_req_1 = []\n",
    "bin_dev_req_2 = []\n",
    "bin_dev_req_3 = []\n",
    "bin_dev_req_4 = []\n",
    "bin_dev_req_5 = []\n",
    "bin_dev_req_6 = []\n",
    "bin_dev_req_7 = []\n",
    "bin_dev_req_8 = []\n",
    "bin_dev_req_9 = []\n",
    "\n",
    "for a,b,c,d,e,f,g,h,i in zip(norm_dev_req['requester_account_age_in_days_at_request'],\n",
    "                             norm_dev_req['requester_days_since_first_post_on_raop_at_request'],\n",
    "                             norm_dev_req['requester_number_of_comments_at_request'],\n",
    "                             norm_dev_req['requester_number_of_comments_in_raop_at_request'],\n",
    "                             norm_dev_req['requester_number_of_posts_at_request'],\n",
    "                             norm_dev_req['requester_number_of_posts_on_raop_at_request'],\n",
    "                             norm_dev_req['requester_number_of_subreddits_at_request'],\n",
    "                             norm_dev_req['requester_upvotes_minus_downvotes_at_request'],\n",
    "                             norm_dev_req['requester_upvotes_plus_downvotes_at_request']):\n",
    "    if a>0:\n",
    "        bin_dev_req_1.append(1)\n",
    "    else:\n",
    "        bin_dev_req_1.append(0)\n",
    "        \n",
    "    if b>0:\n",
    "        bin_dev_req_2.append(1)\n",
    "    else:\n",
    "        bin_dev_req_2.append(0)\n",
    "        \n",
    "    if c>0:\n",
    "        bin_dev_req_3.append(1)\n",
    "    else:\n",
    "        bin_dev_req_3.append(0)\n",
    "        \n",
    "    if d>0:\n",
    "        bin_dev_req_4.append(1)\n",
    "    else:\n",
    "        bin_dev_req_4.append(0)\n",
    "        \n",
    "    if e>0:\n",
    "        bin_dev_req_5.append(1)\n",
    "    else:\n",
    "        bin_dev_req_5.append(0)\n",
    "        \n",
    "    if f>0:\n",
    "        bin_dev_req_6.append(1)\n",
    "    else:\n",
    "        bin_dev_req_6.append(0)\n",
    "        \n",
    "    if g>0:\n",
    "        bin_dev_req_7.append(1)\n",
    "    else:\n",
    "        bin_dev_req_7.append(0)\n",
    "        \n",
    "    if h>0:\n",
    "        bin_dev_req_8.append(1)\n",
    "    else:\n",
    "        bin_dev_req_8.append(0)\n",
    "        \n",
    "    if i>0:\n",
    "        bin_dev_req_9.append(1)\n",
    "    else:\n",
    "        bin_dev_req_9.append(0)\n",
    "                             \n",
    "\n",
    "bin_dev_req = pd.DataFrame()\n",
    "\n",
    "bin_dev_req['requester_account_age_in_days_at_request'] = bin_dev_req_1\n",
    "bin_dev_req['requester_days_since_first_post_on_raop_at_request'] = bin_dev_req_2\n",
    "bin_dev_req['requester_number_of_comments_at_request'] = bin_dev_req_3\n",
    "bin_dev_req['requester_number_of_comments_in_raop_at_request'] = bin_dev_req_4\n",
    "bin_dev_req['requester_number_of_posts_at_request'] = bin_dev_req_5\n",
    "bin_dev_req['requester_number_of_posts_on_raop_at_request'] = bin_dev_req_6\n",
    "bin_dev_req['requester_number_of_subreddits_at_request'] = bin_dev_req_7\n",
    "bin_dev_req['requester_upvotes_minus_downvotes_at_request'] = bin_dev_req_8\n",
    "bin_dev_req['requester_upvotes_plus_downvotes_at_request'] = bin_dev_req_9\n",
    "\n",
    "print 'Requester Random Forest Model (binary, scaled):'\n",
    "Random_Forest(bin_train_req, train_outcomes, bin_dev_req, dev_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal + Requester Random Forest Model:\n",
      "Random Forests Model Accuracy ( 2 estimators ): 71.41 %\n"
     ]
    }
   ],
   "source": [
    "#Combine Temporal & Requester Random Forest Models\n",
    "temp_req_train = pd.concat([train_temporal, bin_train_req], axis=1, join='inner')\n",
    "temp_req_dev = pd.concat([dev_temporal, bin_dev_req], axis=1, join='inner')\n",
    "\n",
    "print 'Temporal + Requester Random Forest Model:'\n",
    "Random_Forest(temp_req_train, train_outcomes, temp_req_dev, dev_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-3-3. Text Random Forest Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forests Model Accuracy ( 12 estimators ): 73.27 %\n"
     ]
    }
   ],
   "source": [
    "print 'Text Random Forest Model (All Features):'\n",
    "Random_Forest(train_text_features, train_outcomes, dev_text_features, dev_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Random Forest Model (Reduced Features):\n",
      "Random Forests Model Accuracy ( 14 estimators ): 71.41 %\n"
     ]
    }
   ],
   "source": [
    "print 'Text Random Forest Model (Reduced Features):'\n",
    "Random_Forest(train_text_reduced, train_outcomes, dev_text_reduced, dev_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *4-3-4. Combined Random Forest Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal + Requester + Text Random Forest Model:\n",
      "Random Forests Model Accuracy ( 16 estimators ): 73.64 %\n"
     ]
    }
   ],
   "source": [
    "train_text_features = pd.DataFrame(train_text_features)\n",
    "dev_text_features = pd.DataFrame(dev_text_features)\n",
    "\n",
    "temp_req_txt_train = pd.concat([temp_req_train, train_text_features], axis=1, join='inner')\n",
    "temp_req_txt_dev = pd.concat([temp_req_dev, dev_text_features], axis=1, join='inner')\n",
    "\n",
    "print 'Temporal + Requester + Text Random Forest Model:'\n",
    "Random_Forest(temp_req_txt_train, train_outcomes, temp_req_txt_dev, dev_outcomes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
